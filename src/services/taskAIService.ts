
import { useLLMService } from '@/hooks/useLLMService';
import { TaskContext } from '@/utils/taskContext';

export interface TaskAISummary {
  statusSummary: string;
  nextSteps: string;
}

export async function generateTaskStateAndSteps(
  context: TaskContext,
  makeLLMRequest: ReturnType<typeof useLLMService>['makeLLMRequest']
): Promise<TaskAISummary> {
  console.log('ğŸ¤– Iniciando generaciÃ³n IA con estructura correcta:', {
    taskTitle: context.mainTask.title,
    taskStatus: context.mainTask.status,
    hasLogs: context.recentLogs.length,
    subtasksCount: context.subtasks.length,
    progress: context.completionStatus.overallProgress
  });

  const systemPrompt = `Eres un asistente experto en gestiÃ³n de tareas que genera resÃºmenes contextuales.

Responde ÃšNICAMENTE en formato JSON vÃ¡lido con esta estructura exacta:
{"statusSummary":"descripciÃ³n del estado actual en mÃ¡ximo 24 palabras","nextSteps":"prÃ³ximas acciones especÃ­ficas en mÃ¡ximo 20 palabras"}

EJEMPLOS:
{"statusSummary":"Proyecto moldes CS1-CS6 en progreso, 7 subtareas completadas (29%), pendiente asignaciÃ³n forwarder","nextSteps":"Asignar forwarder para transporte y completar envÃ­o plantillas pendientes"}

{"statusSummary":"Documentos completados y enviados al equipo, esperando feedback de stakeholders principales","nextSteps":"Agendar reuniÃ³n de revisiÃ³n y consolidar comentarios para segunda versiÃ³n"}`;

  const userPrompt = `INFORMACIÃ“N DE LA TAREA:
Tarea Principal: ${context.mainTask.title}
${context.mainTask.description ? `DescripciÃ³n: ${context.mainTask.description}` : ''}
Estado: ${context.mainTask.status}
Prioridad: ${context.mainTask.priority || 'No definida'}
Progreso General: ${context.completionStatus.overallProgress}%

PROGRESO DETALLADO:
- Subtareas: ${context.completionStatus.completedSubtasks}/${context.completionStatus.totalSubtasks} completadas
- Microtareas: ${context.completionStatus.completedMicrotasks}/${context.completionStatus.totalMicrotasks} completadas

SUBTAREAS ACTUALES:
${context.subtasks.map(st => `â€¢ ${st.title} (${st.status})`).join('\n') || 'No hay subtareas'}

ACTIVIDAD RECIENTE:
${context.recentLogs.slice(0, 5).map(log => 
  `â€¢ ${log.description} (${new Date(log.created_at).toLocaleDateString()})`
).join('\n') || 'Sin actividad reciente'}

Genera resumen del estado actual y prÃ³ximos pasos especÃ­ficos en JSON:`;

  try {
    console.log('ğŸš€ Enviando request con estructura de useAIAssistantSimple');
    
    // âœ… USAR EXACTAMENTE LA MISMA ESTRUCTURA QUE FUNCIONA - SIN maxTokens
    const response = await makeLLMRequest({
      systemPrompt,
      userPrompt,
      functionName: 'task_summary_generation',
      temperature: 0.7
    });

    console.log('ğŸ“¥ Respuesta completa:', response);

    if (!response.content) {
      console.error('âŒ Response sin contenido:', response);
      throw new Error('No content in LLM response');
    }

    console.log('ğŸ“ Contenido IA recibido:', response.content);
    
    const parsed = parseAIResponse(response.content);
    console.log('âœ… JSON parseado:', parsed);
    
    return {
      statusSummary: parsed.statusSummary || "Estado analizado por IA",
      nextSteps: parsed.nextSteps || "Definir prÃ³ximas acciones"
    };

  } catch (error) {
    console.error('âŒ Error en generateTaskStateAndSteps:', error);
    console.log('ğŸ”„ Usando fallback inteligente');
    return generateIntelligentFallback(context);
  }
}

function parseAIResponse(content: string): Partial<TaskAISummary> {
  console.log('ğŸ” Parseando respuesta:', content);
  
  try {
    // Intentar JSON directo primero
    const result = JSON.parse(content.trim());
    console.log('âœ… JSON parseado exitosamente:', result);
    return result;
  } catch (error) {
    console.log('âš ï¸ JSON directo fallÃ³, intentando extracciÃ³n...');
    
    // Buscar JSON en el contenido
    const jsonMatch = content.match(/\{[^}]*\}/);
    if (jsonMatch) {
      try {
        const result = JSON.parse(jsonMatch[0]);
        console.log('âœ… JSON extraÃ­do exitosamente:', result);
        return result;
      } catch (e) {
        console.log('âŒ ExtracciÃ³n JSON fallÃ³');
      }
    }
    
    // Fallback simple
    return {
      statusSummary: content.slice(0, 100) || "Respuesta IA recibida",
      nextSteps: "Continuar con siguiente tarea"
    };
  }
}

function generateIntelligentFallback(context: TaskContext): TaskAISummary {
  const { completionStatus, mainTask, recentLogs } = context;
  
  let statusSummary = "";
  let nextSteps = "";
  
  if (completionStatus.overallProgress >= 90) {
    statusSummary = "Tarea casi completada, en fase final de revisiÃ³n y cierre del proyecto";
    nextSteps = "Revisar detalles finales y confirmar completitud antes de marcar como terminado";
  } else if (completionStatus.overallProgress >= 50) {
    statusSummary = `Progreso significativo alcanzado (${completionStatus.overallProgress}%), desarrollo en curso segÃºn planificaciÃ³n`;
    nextSteps = "Continuar con subtareas pendientes manteniendo el ritmo actual de trabajo";
  } else if (recentLogs.length > 0) {
    const lastLog = recentLogs[0];
    statusSummary = `Actividad reciente: ${lastLog.description.slice(0, 40)}... - trabajo en progreso`;
    nextSteps = "Revisar siguiente subtarea en la lista y definir acciones especÃ­ficas";
  } else if (mainTask.status === 'in_progress') {
    statusSummary = "Tarea iniciada, requiere atenciÃ³n continua para mantener progreso y cumplir objetivos";
    nextSteps = "Identificar primera subtarea prioritaria y establecer plan de trabajo detallado";
  } else {
    statusSummary = "Tarea preparada para iniciar, recursos disponibles y contexto establecido correctamente";
    nextSteps = "Comenzar con primera subtarea de mayor prioridad segÃºn plan establecido";
  }
  
  return { statusSummary, nextSteps };
}

// FunciÃ³n de testing para browser console
declare global {
  interface Window {
    testTaskAIWithCorrectStructure: (taskId: string) => Promise<any>;
  }
}

if (typeof window !== 'undefined') {
  window.testTaskAIWithCorrectStructure = async (taskId: string) => {
    console.log('ğŸ§ª Test con estructura correcta...');
    
    try {
      const { getTaskContext } = await import('@/utils/taskContext');
      const { useLLMService } = await import('@/hooks/useLLLService');
      
      const context = await getTaskContext(taskId);
      console.log('ğŸ“Š Contexto obtenido:', context);
      
      // Usar exactamente la misma estructura que useAIAssistantSimple
      const systemPrompt = "Eres un asistente que responde solo en JSON: {\"test\":\"respuesta exitosa\"}";
      const userPrompt = "Responde con JSON de test confirmando que la estructura funciona";
      
      const { makeLLMRequest } = useLLMService();
      const response = await makeLLMRequest({
        systemPrompt,
        userPrompt,
        functionName: 'test_task_ai',
        temperature: 0.7
      });
      
      console.log('ğŸ¯ Test result:', response);
      return response;
    } catch (error) {
      console.error('âŒ Test error:', error);
      return error;
    }
  };
}
